

note:

for both 1 and 2 below, use the developer vlan, check your env proxy settings :

   $env | grep -i ^http | grep -i _proxy
   
      https_proxy=http://199.19.250.205:80
      http_proxy=http://199.19.250.205:80

------------------

(1) install angular-cli

   (dev vlan)
   $ npm install -g @angular/cli


(2) create new angular project

   (dev vlan)
   $ ng new andy-app
       'y' to add Angular routing
       'SCSS' as stylesheet

   $ cd andy-app
   $ ng serve --open    (open = start browser)



----------------------------------

azure: dev ops

https://dev.azure.com/

   MS asks for some details : 

   name: Andy Grahame
   email : adm_grahama2@admiralgroup.co.uk
   from : UK

----------
My organisations:

Ian has set up a new organisation : admrandd, should appear as one of the orgs
in the left hand list

click on admrandd, should now get redirected to :

   https://dev.azure.com/admrandd




------------

Project: should see claims-tracker project (Ian created this)



--------------

Security

create personal access token

 - click on your personal initials icon (AG in my case) in the top right
   dev-ops bar
 - click on security
 - should now see Security in User Settings menu, under that Personal Access
   token, click that. 
 - Click on New token:

      Name : Andy randd org from dell work laptop
      Organization : admrandd
      Expiration : set a suitable expiry

      Scope : set your scope, for simplicity I set 'Full access', but when u
need tigher control, set it accordingly

      click create

      should see the success panel pop up,

      **SAVE this token smoewhere safe, cannot access it again after this point***  



---------------

clone project locally :

********** (i did this via guest), but need to get it fixed for dev vlan
********** i had to unset any of the proxy env vars, i.e.
**********    $ unset http_proxy
**********    $ unset https_proxy
********** also had to comment out the proxy(s) set in ~/.gitconfig
********** after these changes the clone will succeed on guest

browse to azure dev ops for your project, e.g. 

    https://dev.azure.com/admrandd/_git/claims-tracker

in the left menu, click on Repos, should now see the project file system 

find the 'clone' link at far right of the top menu

click on the HTTPS tab, copy the url

in a command window, create an area where you want to clone the project, then type :

   $ git clone https://admrandd@dev.azure.com/admrandd/claims-tracker/_git/claims-tracker 

replacing the username admrandd with your own azure username, so in my case:

   $ git clone https://adm_grahama2@dev.azure.com/admrandd/claims-tracker/_git/claims-tracker


   this will succeed on guest

--------------------

in WSL 

install azure cli:
   - https://docs.microsoft.com/en-us/cli/azure/install-azure-cli-apt?view=azure-cli-latest

install kubectl:
   - https://kubernetes.io/docs/tasks/tools/install-kubectl/#install-kubectl-on-linux


----------------------

set up the kubectl config to point at our azure cluster

need to retriev resource group & aks cluster name from azure dashboard, and then:

   $ az login
   $ az aks get-credentials --resource-group randd-ukwest-dev-rg --name randd-ukwest-dev-aks

this command can also be retrieved as follows : 

  - log on to azure portal :  https://portal.azure.com/
  - search for aks, select 'kubernetes services'
  - click on your k8s services in the list, e.g. randd-ukwest-dev-aks
  - note there are a few small panels, e.g. 'Monitor Containers', 'View logs' & 'View Kubernetes dashboard'
  - click on 'Kubernetes dashboard', should see a set of steps to connect to the dashboard,
  - step 3, is the get credentials command, and this should be the same as the one i've listed above


!!! if on guest, will need to unset any proxy env vars set up first

--------------------------


made change to etc/hosts - added the following to my c:\Windows\System32\drivers\etc\hosts file:

10.175.38.99   claims-tracker.randd.aks


------------------

kubectl 

set up default namespace for kubectl commands

 kubectl config set-context --current --namespace=claims-tracker-prod

------------------------------

multi-stage pipeline

to be able to see all the individual parts of the pipeline, need to enable the following in azure-dev-ops

in dev-ops, click on personal icon (e.g. AG) 
click  Preview features
enable multi-stage pipelines


--------------------------------------

Get Build image name : 

dev ops site:

   click on pipelines
   click on your pipeline
   find the run that you're interested in, click on it
   in the triggered panel, find the Artifacts column, and click on the '1 published' entry
   click on manifests
   click on deployment.yaml, should download manifests.zip, open this and view the deployment.yaml in an editor
   find the image name, e.g.
          image: randdukwestdevacr.azurecr.io/admiral-claims/claims-tracker-backend:41

----------------------------------------


Create a keyvault : 

followed instrcutions here : https://docs.microsoft.com/en-us/azure/key-vault/quick-create-cli

1. show existing keyvaults

   $ az keyvault list --resource-group "randd-ukwest-dev-rg"
   []


2. create a new vault:

   $ az keyvault create --name "sca-auth-key-vault" --resource-group "randd-ukwest-dev-rg" --location "ukwest"
   {
     "id": "/subscriptions/001c13d7-556f-4ed8-8ccf-4fe4061f5280/resourceGroups/randd-ukwest-dev-rg/providers/Microsoft.KeyVault/vaults/sca-auth-key-vault",
     "location": "ukwest",
     "name": "sca-auth-key-vault",
     "properties": {
       "accessPolicies": [
         {
           "applicationId": null,
           "objectId": "1eaeb4c1-1541-4d21-b09e-83fc027750e0",
           "permissions": {
             "certificates": [
               "get",
               "list",
               "delete",
               "create",
               "import",
               "update",
               "managecontacts",
               "getissuers",
               "listissuers",
               "setissuers",
               "deleteissuers",
               "manageissuers",
               "recover"
             ],
             "keys": [
               "get",
               "create",
               "delete",
               "list",
               "update",
               "import",
               "backup",
               "restore",
               "recover"
             ],
             "secrets": [
               "get",
               "list",
               "set",
               "delete",
               "backup",
               "restore",
               "recover"
             ],
             "storage": [
               "get",
               "list",
               "delete",
               "set",
               "update",
               "regeneratekey",
               "setsas",
               "listsas",
               "getsas",
               "deletesas"
             ]
           },
           "tenantId": "d5b43ab2-065e-4daf-ba23-3348690b5b3a"
         }
       ],
       "createMode": null,
       "enablePurgeProtection": null,
       "enableSoftDelete": null,
       "enabledForDeployment": false,
       "enabledForDiskEncryption": null,
       "enabledForTemplateDeployment": null,
       "networkAcls": null,
       "provisioningState": "Succeeded",
       "sku": {
         "name": "standard"
       },
       "tenantId": "d5b43ab2-065e-4daf-ba23-3348690b5b3a",
       "vaultUri": "https://sca-auth-key-vault.vault.azure.net/"
     },
     "resourceGroup": "randd-ukwest-dev-rg",
     "tags": {},
     "type": "Microsoft.KeyVault/vaults"
   }


3a. add secret to vault

    $ az keyvault secret set --vault-name "sca-auth-key-vault" --name "AndyPassword" --value "Wales_v_Hungary"


3b. view secret from vault

   (1) using az-cli :

         $ az keyvault secret show --name "AndyPassword" --vault-name "sca-auth-key-vault"
         {
           "attributes": {
             "created": "2019-11-19T11:47:55+00:00",
             "enabled": true,
             "expires": null,
             "notBefore": null,
             "recoveryLevel": "Purgeable",
             "updated": "2019-11-19T11:47:55+00:00"
           },
           "contentType": null,
           "id": "https://sca-auth-key-vault.vault.azure.net/secrets/AndyPassword/b1e8dfaf714f48ef96154d26d482c31e",
           "kid": null,
           "managed": null,
           "tags": {
             "file-encoding": "utf-8"
           },
           "value": "Wales_v_Hungary"
         }

   (2) use secret's URI: 

             https://sca-auth-key-vault.vault.azure.net/secrets/AndyPassword

         returns :

             {"error":{"code":"Unauthorized","message":"Request is missing a Bearer or PoP token."}}


         TODO - describe how to get this to work



4. add cert/key files to vault
         TODO - describe how to get this to work


5. use key/cert from vault in pipeline
         TODO - describe how to get this to work


6. delete a vault : 

  $ az keyvault delete --name "sca-auth-key-vault"

   
--------------------------------------------

secure files in azure:

see : https://docs.microsoft.com/en-us/azure/devops/pipelines/library/secure-files?view=azure-devops














 kubectl -n sca-auth attach sca-auth-deployment-64bd5d9f94-kqsx2 -i
 kubectl -n sca-auth delete deployment.apps/sca-auth-deployment
 kubectl -n sca-auth delete deployment.apps/sca-auth-deployment replicaset.apps/sca-auth-deployment-7cd688cf7
 kubectl -n sca-auth delete deployment.apps/sca-auth-deployment service/sca-auth-service pod/sca-auth-deployment-6cf4cdb778-tnx47
 kubectl -n sca-auth delete deployment.apps/sca-auth-deployment service/sca-auth-service pod/sca-auth-deployment-7c8586c8ff-t5fvg
 kubectl -n sca-auth delete deployment.apps/sca-auth-deployment service/sca-auth-service pod/sca-auth-deployment-7d9b756856-2zsnl
 kubectl -n sca-auth delete deployment.apps/sca-auth-deployment service/sca-auth-service pod/sca-auth-deployment-db6cbc4c9-t45vv
 kubectl -n sca-auth delete pod pod/sca-auth-deployment-64bd5d9f94-29dzr pod/sca-auth-deployment-b5f4c4bdd-s5sfp
 kubectl -n sca-auth delete pod sca-auth-deployment-64bd5d9f94-2566n sca-auth-deployment-b5f4c4bdd-st72j
 kubectl -n sca-auth delete pod sca-auth-deployment-64bd5d9f94-6rx7f sca-auth-deployment-b5f4c4bdd-qhhcg
 kubectl -n sca-auth delete pod sca-auth-deployment-64bd5d9f94-jjw9h sca-auth-deployment-b5f4c4bdd-snp65
 kubectl -n sca-auth delete pod sca-auth-deployment-64bd5d9f94-kqsx2
 kubectl -n sca-auth delete pod sca-auth-deployment-b5f4c4bdd-n7hv5
 kubectl -n sca-auth delete pod/sca-auth-deployment-64bd5d9f94-29dzr pod/sca-auth-deployment-b5f4c4bdd-s5sfp
 kubectl -n sca-auth delete sca-auth-deployment-64bd5d9f94-2566n sca-auth-deployment-b5f4c4bdd-st72j
 kubectl -n sca-auth delete service/sca-auth-service
 kubectl -n sca-auth delete service/sca-auth-service pod/sca-auth-deployment-7cd688cf7-4rs4l
 kubectl -n sca-auth describe pod sca-auth-deployment-64bd5d9f94-kqsx2
 kubectl -n sca-auth describe pod sca-auth-deployment-b5f4c4bdd-n7hv5
 kubectl -n sca-auth exec -it pod/sca-auth-deployment-565b44cb68-dtrbp -- frerfe
 kubectl -n sca-auth exec -it pod/sca-auth-deployment-565b44cb68-dtrbp -- lsof -i -P -n | grep LISTEN
 kubectl -n sca-auth exec -it pod/sca-auth-deployment-565b44cb68-dtrbp -- lsof -i:8080
 kubectl -n sca-auth exec -it pod/sca-auth-deployment-565b44cb68-dtrbp -- netstat -tulpn | grep LISTEN
 kubectl -n sca-auth exec -it sca-auth-deployment-64bd5d9f94-kqsx2 -- ls -la /tmp/healthy
 kubectl -n sca-auth exec -it sca-auth-deployment-64bd5d9f94-kqsx2 -- sh
 kubectl -n sca-auth exec -it sca-auth-deployment-7cd688cf7-4rs4l -- ls -la /tmp/healthy
 kubectl -n sca-auth exec -it sca-auth-deployment-7cd688cf7-4rs4l -- lsof -i -P -n | grep LISTEN
 kubectl -n sca-auth exec -it sca-auth-deployment-7cd688cf7-4rs4l -- nc 8080
 kubectl -n sca-auth exec -it sca-auth-deployment-b5f4c4bdd-n7hv5 -- ls -la /tmp/healthy
 kubectl -n sca-auth exec -it sca-auth-mock-insurance-api-deployment-9fc4fc56f-qqprl -- cat /etc/hosts
 kubectl -n sca-auth exec -it sca-auth-mock-insurance-api-deployment-9fc4fc56f-qqprl -- ls -l /etc/hosts
 kubectl -n sca-auth exec -it sca-auth-mock-insurance-api-deployment-9fc4fc56f-qqprl -- ls -la /tmp/healthy
 kubectl -n sca-auth get all
 kubectl -n sca-auth get pods
 kubectl -n sca-auth get pods; kubectl -n sca-auth logs -f sca-auth-deployment-64bd5d9f94-29dzr
 kubectl -n sca-auth get pods; kubectl -n sca-auth logs -f sca-auth-deployment-b5f4c4bdd-s5sfp
 kubectl -n sca-auth get services
 kubectl -n sca-auth logs -f pod/sca-auth-deployment-7cd688cf7-4rs4l
 kubectl -n sca-auth logs -f pod/xxx
 kubectl -n sca-auth logs -f sca-auth-deployment-64bd5d9f94-2566n 
 kubectl -n sca-auth logs -f sca-auth-deployment-64bd5d9f94-29dzr
 kubectl -n sca-auth logs -f sca-auth-deployment-64bd5d9f94-6rx7f
 kubectl -n sca-auth logs -f sca-auth-deployment-64bd5d9f94-kqsx2
 kubectl -n sca-auth logs -f sca-auth-deployment-b5f4c4bdd-qhhcg
 kubectl -n sca-auth logs -f sca-auth-deployment-b5f4c4bdd-s5sfp
 kubectl -n sca-auth logs -f sca-auth-deployment-b5f4c4bdd-st72j
 kubectl -n sca-auth logs pod/sca-auth-deployment-db6cbc4c9-t45vv
 kubectl -n sca-auth logs sca-auth-deployment-64bd5d9f94-kqsx2
 kubectl -n sca-auth logs sca-auth-deployment-b5f4c4bdd-n7hv5
 kubectl apply -f .
 kubectl apply -f ./k8s
 kubectl apply -f deployment.yaml 
 kubectl apply -f k8s/
 kubectl apply -f k8s/deployment.yaml 
 kubectl apply .
 kubectl attach sca-auth-deployment-64bd5d9f94-kqsx2 -i
 kubectl config current-context
 kubectl config use-context docker-desktop 
 kubectl config view
 kubectl create -f azure-pipelines.yml 
 kubectl create -f k8s/deployment.yaml 
 kubectl create -f k8s/ingress.yaml 
 kubectl create -f k8s/service.yaml 
 kubectl delete -f ./k8s
 kubectl delete -f k8s/deployment.yaml 
 kubectl delete service/kubernetes
 kubectl delete service/kubernetes service/sca-auth-service
 kubectl delete service/sca-auth-service
 kubectl describe pod/sca-auth-deployment-78c8557d76-2ktz7
 kubectl describe sca-auth-ingress
 kubectl exec -it -n sca-auth sca-auth-deployment-64bd5d9f94-kqsx2 -- sh
 kubectl exec -it sca-auth-deployment-64bd5d9f94-kqsx2 -- /bin/bash
 kubectl exec -it sca-auth-deployment-64bd5d9f94-kqsx2 -- /bin/bash -n sca-auth
 kubectl exec -it sca-auth-deployment-64bd5d9f94-kqsx2 -- sh
 kubectl exec -it sca-auth-deployment-64bd5d9f94-kqsx2 --namespace sca-auth -- sh
 kubectl exec -it sca-auth-deployment-64bd5d9f94-kqsx2 --namespace=sca-auth -- sh
 kubectl exec -it sca-auth-deployment-64bd5d9f94-kqsx2 -n sca-auth -- sh
 kubectl exec -it sca-auth-mock-insurance-api-deployment-9fc4fc56f-qqprl -- /bin/bash
 kubectl exec -it sca-auth-mock-insurance-api-deployment-9fc4fc56f-qqprl -- ls -l /etc/hosts
 kubectl get all
 kubectl get all --all-namespaces
 kubectl get all --all-namespaces | grep sca-auth
 kubectl get containers -n sca-auth
 kubectl get ingresses
 kubectl get pod sca-auth-deployment-64bd5d9f94-kqsx2
 kubectl get pod sca-auth-deployment-64bd5d9f94-kqsx2 --namespace sca-auth
 kubectl get pod sca-auth-deployment-64bd5d9f94-kqsx2 -n sca-auth
 kubectl get pods
 kubectl get pods -n sca-auth
 kubectl get services --all-namespaces
 kubectl get services -n sca-auth
 kubectl list
 kubectl list pods
 kubectl logs -f sca-auth-deployment-64bd5d9f94-kqsx2 -n sca-auth
 kubectl logs pod/sca-auth-deployment-55dd46464c-fntl7
 kubectl logs sca-auth-deployment-64bd5d9f94-kqsx2
 kubectl logs sca-auth-deployment-64bd5d9f94-kqsx2 -n sca-auth
 kubectl version --short

>>  git clone https://adm_grahama2@dev.azure.com/admrandd/claims-tracker/_git/claims-tracker
Cloning into 'claims-tracker'...
Password for 'https://adm_grahama2@dev.azure.com':


remote: Azure Repos
remote: Found 58 objects to send. (2 ms)
Unpacking objects: 100% (58/58), done.


>>  git clone https://adm_grahama2@dev.azure.com/admrandd/claims-tracker/_git/claims-tracker
Cloning into 'claims-tracker'...
Password for 'https://adm_grahama2@dev.azure.com':



fatal: Authentication failed for 'https://adm_grahama2@dev.azure.com/admrandd/claims-tracker/_git/claims-tracker/'
repos

pipeline
   defined in azure-pipelines.yml

   ci trigger  - see https://docs.microsoft.com/en-us/azure/devops/pipelines/build/triggers?view=azure-devops&tabs=yaml#ci-triggers
      - so if changes are commited to the branches listed in the triger section, the pipeline is triggered 
      - can also set up for tags.
      - can set batch flag to true - to avoid builds being kicked off when a build is still running
      - can include/exclude specific paths - e.g. don't rebuild if a README changes
      - can also turn off triggers - if you want manual control

   pr trigger - pull request
      - cause a build to run whenever a pull request is opened with one of the specified target branches, or when changes are pushed to such a pull request



   breakdown the pipeline into stages, e.g.

      1. Build - create dockerfile(s)
          can then modularize the pipeline e.g. create a yaml file that looks fter this stage
          means that stages can easily be reused
      2. Publish - 
      3. Deploy - 

can define 
Azure dev-ops demo :

by Larissa (Van)

-----------------------------------


create a branch

make code change - check in

can then see a 'create a pull request' link in your window - this means merge back to master

can then add code reviewers to ensure its reviewed

can also choose to remove branch after merge to master

-----------

useful step-by-step guides in confluence Van - Azure migration ....

   - https://confluence.admiral.uk:8443/display/GLAD/Step-by-Step+Guides


-----------

all done in azure-pipelines.yml

   see trigger on what branches trigger the pipeline, e.g. master

   - larissa used a custom agent to cut down build times - see the appropriate step-by-step guide 

for branch specific tasks (different to master) create a new yaml (alongside azure-pipelines.yml)  which does the branch spefici stuff

---------------

creating releases

in set up of the yaml, can add a replace-tokens stage that pulls secret data from vault and replace in yout config, and then continue the release with the real values.

---------------------------------------------

Switching between azure aks k8s and local docker desktop k8s

(1) use aks - 

    (a)  $  unset https_proxy http_proxy ftp_proxy
    (b)  $  az login
         '\\wsl$\Ubuntu-18.04\home\agrahame'
         CMD.EXE was started with the above path as the current directory.
         UNC paths are not supported.  Defaulting to Windows directory.
         Note, we have launched a browser for you to login. For old experience with device code, use "az login --use-device-code"

     a browser tab opens up, select your azure devops account : adm_Grahama2, Andrew
                                                               adm_grahama2@admiralgroup.co.uk

                                                               if required, password for this is stored in Dashlane, secure notes (Application password: Azure Dev ops)

     then , back in the console, the following is outputted:

         You have logged in. Now let us find all the subscriptions to which you have access...
         [
           {
             "cloudName": "AzureCloud",
             "id": "001c13d7-556f-4ed8-8ccf-4fe4061f5280",
             "isDefault": true,
             "name": "AdmiralCloudDevelopment",
             "state": "Enabled",
             "tenantId": "d5b43ab2-065e-4daf-ba23-3348690b5b3a",
             "user": {
               "name": "adm_grahama2@admiralgroup.co.uk",
               "type": "user"
             }
           }
         ]

                                                               
     (c)  $ az aks get-credentials --resource-group randd-ukwest-dev-rg --name randd-ukwest-dev-aks
               /home/agrahame/.kube/config has permissions "777".
               It should be readable and writable only by its owner.
               Merged "randd-ukwest-dev-aks" as current context in /home/agrahame/.kube/config
     

           NOTE: this command can also be retrieved as follows : 

           - log on to azure portal :  https://portal.azure.com/
           - search for aks, select 'kubernetes services'
           - click on your k8s services in the list, e.g. randd-ukwest-dev-aks
           - note there are a few small panels, e.g. 'Monitor Containers', 'View logs' & 'View Kubernetes dashboard'
           - click on 'Kubernetes dashboard', should see a set of steps to connect to the dashboard,
           - step 3, is the get credentials command, and this should be the same as the one i've listed above

      (d)  $ kubectl config current-context
           randd-ukwest-dev-aks


      **** NOW you have set up your local kubectl to connect to your AKS K8S cluster


      (e)  $ kubectl get pods --all-namespaces
               NAMESPACE             NAME                                                     READY   STATUS             RESTARTS   AGE
               claims-tracker-prod   claims-tracker-backend-deployment-68dfd7dc44-979z7       1/1     Running            0          44d
               claims-tracker-prod   claims-tracker-deployment-5df6bd8cbc-dnwht               1/1     Running            0          22d
               ingress-nginx         nginx-ingress-controller-6df4d8b446-hcl9p                1/1     Running            1          118d
               kube-system           azure-cni-networkmonitor-9jmvs                           1/1     Running            0          14d
               kube-system           azure-cni-networkmonitor-htf8z                           1/1     Running            0          14d


               you can check the cluster data (e.g. pods above) to see that you're connected to the right k8s cluster.


(2) use local k8s -

     (a) make sure docker desktop is up and running

     (b) check the possible contexts

            $  kubectl config get-contexts
               CURRENT   NAME                   CLUSTER                AUTHINFO                                               NAMESPACE
                         docker-desktop         docker-desktop         docker-desktop                                         
               *         randd-ukwest-dev-aks   randd-ukwest-dev-aks   clusterUser_randd-ukwest-dev-rg_randd-ukwest-dev-aks   

     (c) can see the current context in the above, but can also use this :

            $  kubectl config current-context
            randd-ukwest-dev-aks
     
     
     (d) select the docker desktop context

            $  kubectl config use-context docker-desktop
            Switched to context "docker-desktop".

     (e) confirm context has switched : 

            $  kubectl config current-context
            docker-desktop

            or

            $  kubectl config get-contexts
            CURRENT   NAME                   CLUSTER                AUTHINFO                                               NAMESPACE
            *         docker-desktop         docker-desktop         docker-desktop                                         
                      randd-ukwest-dev-aks   randd-ukwest-dev-aks   clusterUser_randd-ukwest-dev-rg_randd-ukwest-dev-aks   

     (f) connect to your cluster, e.g. 

            $  kubectl get pods --all-namespaces
            NAMESPACE     NAME                                     READY   STATUS    RESTARTS   AGE
            default       sca-auth-deployment-78c8557d76-6klzv     1/1     Running   0          18h
            docker        compose-6c67d745f6-nm8n8                 1/1     Running   0          22h
            docker        compose-api-57ff65b8c7-bjqwp             1/1     Running   0          22h
            kube-system   coredns-6dcc67dcbc-jzjkn                 1/1     Running   0          22h
            kube-system   coredns-6dcc67dcbc-kx4dk                 1/1     Running   0          22h
            kube-system   etcd-docker-desktop                      1/1     Running   0          22h
            kube-system   kube-apiserver-docker-desktop            1/1     Running   0          22h
            kube-system   kube-controller-manager-docker-desktop   1/1     Running   0          22h
            kube-system   kube-proxy-p6nkk                         1/1     Running   0          22h
            kube-system   kube-scheduler-docker-desktop            1/1     Running   0          22h


           


-------------------------------

Set up local cluster:

using our SCA auth project (adm_grahama2@dev.azure.com/admrandd/sca-auth/_git/sca-auth) as an example.

we have a docker folder containing just the Dockerfile

before we can kick off the deployment in k8s, we need to make sure the docker image it expects has been built and loaded to the local image registry.

so.


step 1 : run the configure.sh script against the vanilla frontend.conf - to insert the okta discovery uri, so that the docker image is built with the okta config already baked in.

   pre-step : nstall jq to your local dev machine
   $  sudo apt-get install jq

   cd to sca-auth/nginx/nginx-openid-connect
   should see configure.sh and frontend.conf among files here

   NOTE: for the following command, you need to use your own client-id and client-secret here - check with your IdP 

   $ ./configure.sh -k request -i "0oa1mjs256xq3tKmN357" -s "XaewflSG_nlmMm07971BHd69Qj-fTrqdQXtN2xwd"  https://dev-424995.okta.com/oauth2/default/.well-known/openid-configuration
configure.sh: NOTICE: Configuring /home/agrahame/dev/git-repos/azure-repos/sca-auth/nginx/nginx-openid-connect/frontend.conf
configure.sh: NOTICE: Success - test configuration with 'nginx -t'

   should now find the original frontend.conf file renamed to frontend.conf.ORIG and the new version of frontend.conf contains the okta config defined during the configure script execution.


   
step 2 ; build the docker image

   NOTE: You need to ensure your nginx-repo.crt/key files (received when starting your nginx+ trial) are in the <azure-repos>/sca-auth directory

   $ cd <azure-repos>/sca-auth

   $ docker build -f docker/Dockerfile --no-cache -t sca/sca-auth:1.0 .
   docker build -f docker/Dockerfile --no-cache -t sca/sca-auth:1.0 .
   Sending build context to Docker daemon  565.8kB
   Step 1/16 : FROM debian:stretch-slim
    ---> c2f145c34384
   Step 2/16 : COPY nginx-repo.crt /etc/ssl/nginx/
    ---> a074a87b801d
   Step 3/16 : COPY nginx-repo.key /etc/ssl/nginx/
    ---> bab66563b5da
         :       :       :
         :       :       :
         :       :       :
   Step 15/16 : STOPSIGNAL SIGTERM
    ---> Running in 3ee7ace3edb1
   Removing intermediate container 3ee7ace3edb1
    ---> 893b41f8523b
   Step 16/16 : CMD ["nginx", "-g", "daemon off;"]
    ---> Running in 41d0480135bf
   Removing intermediate container 41d0480135bf
    ---> d5f364b17b99
   Successfully built d5f364b17b99
   Successfully tagged sca/sca-auth:1.0


This should build the image, fix any errors (e.g. might need some files to copied into your working directory  during docker build process).

to check its been built and uploaded to your registry:

   $  docker image list
   REPOSITORY                           TAG                 IMAGE ID            CREATED              SIZE
   sca/sca-auth                         1.0                 d5f364b17b99        About a minute ago   82.1MB
   <none>                               <none>              139c8cafa7e9        19 hours ago         82.1MB

   NOTE: seemed to have a problem when not supplying a version, i.e. use 'latest' so in the end we added v 1.0 to the docker build.


Now need to modify any yaml files that need to use this docker image, e.g. edit k8s/deployment.yaml:

-------------------------------------------------------
         apiVersion: apps/v1
         kind: Deployment
         metadata:
           name: sca-auth-deployment
           labels: 
             app.kubernetes.io/name: sca-auth-deployment
         spec:
           replicas: 1
           selector:
             matchLabels:
               app.kubernetes.io/name: sca-auth-deployment
           template:
             metadata:
               labels:
                 app.kubernetes.io/name: sca-auth-deployment
             spec:   
               containers:
                 - name: sca-auth
                   image: sca/sca-auth:1.0
                   ports:
                     - name: http
                       containerPort: 80
                       protocol: TCP
                   livenessProbe:
                     httpGet:
                       path: /nginx-health
                       port: http
                   readinessProbe:
                     httpGet:
                       path: /nginx-health
                       port: http
-------------------------------------------------------

notice, that the version for the sca/sca-auth image is changed to 1.0 

we're now ready to apply out k8s yaml files : 

we have a k8s directory containing :

   deployment.yaml
   ingress.yaml
   service.yaml


there's also :

azure-pipelines.yml

in the project root, but that's only used when setting up a pipeline in Azure-AKS.

we'll have to do the pipeline manually, but can still use the config in the k8s/*yaml files to set up the k8s resources, e.g. 

   $ kubectl apply -f k8s/
   deployment.apps/sca-auth-deployment created
   ingress.extensions/sca-auth-ingress created
   service/sca-auth-service created
   

   or individually : 

   $  kubectl apply -f k8s/deployment.yaml 
   deployment.apps/sca-auth-deployment created

   $  kubectl apply -f k8s/ingress.yaml 
   ingress.extensions/sca-auth-ingress created

   $  kubectl apply -f k8s/service.yaml 
   service/sca-auth-service created




   and can also clean up using these files

   $  kubectl delete -f k8s/
   deployment.apps "sca-auth-deployment" deleted
   ingress.extensions "sca-auth-ingress" deleted
   service "sca-auth-service" deleted

   or (again) individually:

   $  kubectl delete -f k8s/deployment.yaml 
   deployment.apps "sca-auth-deployment" deleted

   $  kubectl delete -f k8s/ingress.yaml 
   ingress.extensions "sca-auth-ingress" deleted

   $  kubectl delete -f k8s/service.yaml 
   service "sca-auth-service" deleted





----------------------------

build landing page docker image

step 1 ; build the docker image

   $ cd <azure-repos>/sca-auth-mock-landing-page

   $ docker build -f docker/Dockerfile --no-cache -t sca/sca-auth-mock-landing-page:1.0 .


step 2 : update deployment yaml

   edit the k8s/deployment.yaml file to use the newly built docker image tag:

   and change the following line : 

-          image: #{dockerImage}#
+          image: sca/sca-auth-mock-landing-page:1.0


step 3 : apply the k8s 

    $ kubectl apply -f k8s/
    deployment.apps/sca-auth-mock-landing-page-deployment created
    service/sca-auth-mock-landing-page-service created




----------------------------

build mock insurance-app image

step 1 : build the app

   $ cd <azure-repos>/sca-auth-mock-insurance-api
   $ mvn package


step 1 ; build the docker image

   $ cd <azure-repos>/sca-auth-mock-insurance-api

   $ docker build -f docker/Dockerfile --no-cache -t sca/sca-auth-mock-insurance-api:1.0 .


step 2 : update deployment yaml

   edit the k8s/deployment.yaml file to use the newly built docker image tag:

   and change the following line : 

-          image: #{dockerImage}#
+          image: sca/sca-auth-mock-landing-page:1.0


step 3 : apply the k8s 

    $ kubectl apply -f k8s/
    deployment.apps/sca-auth-mock-landing-page-deployment created
    service/sca-auth-mock-landing-page-service created



-------------------------------------------------------

if running locally on your dev machine using docker-desktop, need to set ingress controller/service

taken from : https://kubernetes.github.io/ingress-nginx/deploy/

(a) first part - mandatory setup 
   $  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/mandatory.yaml
   namespace/ingress-nginx created
   configmap/nginx-configuration created
   configmap/tcp-services created
   configmap/udp-services created
   serviceaccount/nginx-ingress-serviceaccount created
   clusterrole.rbac.authorization.k8s.io/nginx-ingress-clusterrole created
   role.rbac.authorization.k8s.io/nginx-ingress-role created
   rolebinding.rbac.authorization.k8s.io/nginx-ingress-role-nisa-binding created
   clusterrolebinding.rbac.authorization.k8s.io/nginx-ingress-clusterrole-nisa-binding created
   deployment.apps/nginx-i ngress-controller created


(b) second part - create the ingress service :
   $  kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml
   service/ingress-nginx created


confirm its started .... 

   $  kubectl get pods --all-namespaces -l app.kubernetes.io/name=ingress-nginx
   NAMESPACE       NAME                                        READY   STATUS    RESTARTS   AGE
   ingress-nginx   nginx-ingress-controller-7dcc95dfbf-c987n   1/1     Running   0          3m

   
   $ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/master/deploy/static/provider/cloud-generic.yaml



-----------------------------------

before we can hit out ingress, we need to add an entry into our hosts file:

(1) find our ingress

      kubectl describe ingresses
      Name:             sca-auth-ingress
      Namespace:        default
      Address:          localhost
      Default backend:  default-http-backend:80 (<none>)
      Rules:
        Host            Path  Backends
        ----            ----  --------
        sca-auth.local  
                           sca-auth-service:80 (10.1.0.35:80)
      Annotations:
        kubectl.kubernetes.io/last-applied-configuration:  {"apiVersion":"extensions/v1beta1","kind":"Ingress","metadata":{"annotations":{},"name":"sca-auth-ingress","namespace":"default"},"spec":{"rules":[{"host":"sca-auth.local","http":{"paths":[{"backend":{"serviceName":"sca-auth-service","servicePort":80}}]}}]}}

      Events:
        Type    Reason  Age   From                      Message
        ----    ------  ----  ----                      -------
        Normal  CREATE  41m   nginx-ingress-controller  Ingress default/sca-auth-ingress
        Normal  UPDATE  13m   nginx-ingress-controller  Ingress default/sca-auth-ingress


(2) note the host name from the above output, i.e. sca-auth.local

(3) add an entry to your base hosts file, i.e. Windows/System32/drivers/etc/hosts

         127.0.0.1 sca-auth.local


---------------------------------------------------------

Set up your IdP ro redirect to the ingress

in my case, I've been using OKTA as the IdP, so log on to your IdP admin area, and modify the URI where the IdP will send the OAuth responses.

e.g. 

Login redirect URIs  :   http://sca-auth.local:80/_codexch
Initiate login URI   :   http://sca-auth.local:80/_codexch


--------------------------------------------------------

Now try to hit the ingress : 

    browse to http://sca-auth.local:80

    should be directed to okta to login ....





-------------------------------------------------------------------------------------

Troubleshooting 

Bad Gateway/DNS network issues

I found that nginx may be caching data, so there may be a stale IP floating around, so by deleting the nginx pod (e.g. sca-auth in our case), then this will delete the cache and then alow the nginx to re-build its set of addresses...


Useful tips to find network data - busybox


To help find network/dns problems in your kubernetes cluster

install busybox: 

    $  kubectl apply -f https://k8s.io/examples/admin/dns/busybox.yaml
    pod/busybox created


can then use busybox to run commands, e.g. nslookup to help find useful info


    $  kubectl exec -it busybox -- nslookup sca-auth-mock-landing-page-service 
	Server:    10.96.0.10
	Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local

	Name:      sca-auth-mock-landing-page-service
	Address 1: 10.106.182.104 sca-auth-mock-landing-page-service.default.svc.cluster.local



then can use this to ensure all ip/network config is good:

a) list services 

      $  kubectl get services
		NAME                                  TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)   AGE
		kubernetes                            ClusterIP   10.96.0.1        <none>        443/TCP   5d
		sca-auth-mock-insurance-api-service   ClusterIP   10.103.108.3     <none>        80/TCP    19m
		sca-auth-mock-landing-page-service    ClusterIP   10.106.182.104   <none>        80/TCP    20m
		sca-auth-mock-loans-api-service       ClusterIP   10.103.34.97     <none>        80/TCP    19m
		sca-auth-service                      ClusterIP   10.98.43.2       <none>        80/TCP    20m


can see that the IP for sca-auth-mock-landing-page-service is 10.106.182.104 and that matches the ip returned from the busybox command above

Service Endpoints

   Can also get further info for your service by describing it: 

      $  kubectl describe service sca-auth-mock-landing-page-service
		Name:              sca-auth-mock-landing-page-service
		Namespace:         default
		Labels:            app.kubernetes.io/name=sca-auth-mock-landing-page-service
		Annotations:       kubectl.kubernetes.io/last-applied-configuration:
				     {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"labels":{"app.kubernetes.io/name":"sca-auth-mock-landing-page-service"},...
		Selector:          app.kubernetes.io/name=sca-auth-mock-landing-page-deployment
		Type:              ClusterIP
		IP:                10.106.182.104
		Port:              http  80/TCP
		TargetPort:        http/TCP
		Endpoints:         10.1.0.112:80
		Session Affinity:  None
		Events:            <none>

      note the endpoints defined for this service : 10.1.0.112


then take a look at the pod(s) of this service, and can see that the IP of this pod is (one of) the end points for the service. 

      $  kubectl describe pod/sca-auth-mock-landing-page-deployment-54c586d6fc-rhbks
		Name:           sca-auth-mock-landing-page-deployment-54c586d6fc-rhbks
		Namespace:      default
		Priority:       0
		Node:           docker-desktop/192.168.65.3
		Start Time:     Mon, 25 Nov 2019 14:10:15 +0000
		Labels:         app.kubernetes.io/name=sca-auth-mock-landing-page-deployment
				pod-template-hash=54c586d6fc
		Annotations:    <none>
		Status:         Running
		IP:             10.1.0.112
		Controlled By:  ReplicaSet/sca-auth-mock-landing-page-deployment-54c586d6fc
		Containers:
		  sca-auth-mock-landing-page:
		    Container ID:   docker://e3c122771e090b39b98092ff041a61c5044eae0676e43e6ec188587792b5aa34
		    Image:          sca/sca-auth-mock-landing-page:1.0
		    Image ID:       docker://sha256:954cdf595ff5ce5b2a61a7431d6157b0e1b9a5f160cbce5b012008912bd94d66
		    Port:           80/TCP
		    Host Port:      0/TCP
		    State:          Running
		      Started:      Mon, 25 Nov 2019 14:10:17 +0000
		    Ready:          True
		    Restart Count:  0
		    Liveness:       http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
		    Readiness:      http-get http://:http/ delay=0s timeout=1s period=10s #success=1 #failure=3
		    Environment:    <none>
		    Mounts:
		      /var/run/secrets/kubernetes.io/serviceaccount from default-token-g72dl (ro)
		Conditions:
		  Type              Status
		  Initialized       True 
		  Ready             True 
		  ContainersReady   True 
		  PodScheduled      True 
		Volumes:
		  default-token-g72dl:
		    Type:        Secret (a volume populated by a Secret)
		    SecretName:  default-token-g72dl
		    Optional:    false
		QoS Class:       BestEffort
		Node-Selectors:  <none>
		Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s
				 node.kubernetes.io/unreachable:NoExecute for 300s
		Events:
		  Type    Reason     Age   From                     Message
		  ----    ------     ----  ----                     -------
		  Normal  Scheduled  21m   default-scheduler        Successfully assigned default/sca-auth-mock-landing-page-deployment-54c586d6fc-rhbks to docker-desktop
		  Normal  Pulled     21m   kubelet, docker-desktop  Container image "sca/sca-auth-mock-landing-page:1.0" already present on machine
		  Normal  Created    21m   kubelet, docker-desktop  Created container sca-auth-mock-landing-page
		  Normal  Started    21m   kubelet, docker-desktop  Started container sca-auth-mock-landing-page





